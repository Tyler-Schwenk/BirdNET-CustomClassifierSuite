# ğŸ¸ BirdNET-CustomClassifierSuite

A modular, reproducible pipeline for training and evaluating **custom BirdNET classifiers**  
(e.g., for species detection like California Red-legged Frog, Bullfrog, etc.).

This suite automates every stage of the workflow â€” from data packaging and model training  
to inference, evaluation, and multi-config sweep generation.

---

## ğŸ“¦ Repository Overview

```
BirdNET-CustomClassifierSuite/
â”‚
â”œâ”€â”€ birdnet_custom_classifier_suite/   â† Python package
â”‚   â”œâ”€â”€ pipeline/                      â† Core training + inference pipeline
â”‚   â”œâ”€â”€ sweeps/                        â† Sweep generation + batch execution
â”‚   â”œâ”€â”€ eval_toolkit/                  â† Aggregation and review utilities
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ base.yaml                      â† Global defaults (shared across stages)
â”‚   â”œâ”€â”€ sweep_specs/                   â† Tracked sweep definitions (YAML)
â”‚   â”‚   â”œâ”€â”€ example_sweep.yaml
â”‚   â”‚   â””â”€â”€ test_sweep.yaml
â”‚   â””â”€â”€ sweeps/                        â† Generated sweeps (ignored by Git)
â”‚       â”œâ”€â”€ test_sweep/
â”‚       â”‚   â”œâ”€â”€ stage0_001.yaml
â”‚       â”‚   â”œâ”€â”€ ...
â”‚       â”‚   â””â”€â”€ manifest.csv
â”‚
â”œâ”€â”€ experiments/                       â† Pipeline outputs (models, evals)
â”œâ”€â”€ scripts/                           â† Environment and utility scripts
â”‚   â””â”€â”€ setup_env.ps1
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Environment Setup (Windows)

```powershell
# 1. Create and activate venv
powershell -ExecutionPolicy Bypass -File scripts/setup_env.ps1

# 2. Activate later sessions manually
.\.venv\Scripts\Activate.ps1
```

This script installs all Python dependencies, clones BirdNET-Analyzer locally under `external/`,  
and installs this project in editable mode.

---

## ğŸ§© Base Configuration

Global experiment defaults are stored in [`config/base.yaml`](config/base.yaml).  
These settings apply to all runs and can be overridden by sweep-specific configs.

Example:

```yaml
training:
  epochs: 50
  batch_size: 32
training_args:
  fmin: 0
  fmax: 15000
  overlap: 0.0
  hidden_units: 512
  dropout: 0.25
  learning_rate: 0.0005
  label_smoothing: true
  mixup: true
analyzer_args:
  fmin: 0
  fmax: 15000
  overlap: 0.0
  sensitivity: 1.0
```

âš ï¸ *Important:*  
`fmin`, `fmax`, and `overlap` must match between `training_args` and `analyzer_args`
to ensure consistent spectrogram processing across training and inference.

---

## ğŸ§® Sweep Specs

Tracked sweep definitions live under [`config/sweep_specs/`](config/sweep_specs/).  
Each spec describes:
- the **axes** of parameters to vary
- the **base_params** shared across all configs
- the **stage number** and output directory

Example (`config/sweep_specs/example_sweep.yaml`):

```yaml
stage: 1
out_dir: "config/sweeps/example_sweep"
axes:
  hidden_units: [0, 128, 512]
  dropout: [0.0, 0.25]
  learning_rate: [0.0001, 0.0005, 0.001]
  batch_size: [16, 32]
  seed: [123]
base_params:
  epochs: 50
  upsampling_ratio: 0.0
  mixup: false
  label_smoothing: false
  focal_loss: false
```

---

## ğŸš€ Generating Sweeps

Run the generator with any sweep spec:

```powershell
python -m birdnet_custom_classifier_suite.sweeps.sweep_generator --spec config/sweep_specs/example_sweep.yaml
```

This creates a folder of YAML configs and a manifest CSV under `config/sweeps/<name>/`.

---

## ğŸ§  Running Sweeps

Execute all configs in a sweep folder using the training pipeline:

```powershell
python -m birdnet_custom_classifier_suite.sweeps.run_sweep config/sweeps/example_sweep --base-config config/base.yaml --verbose
```

Each config will:
1. Build its training package
2. Train a BirdNET model
3. Run inference (IID + OOD)
4. Evaluate metrics and update the master experiment index

Outputs appear in `experiments/<experiment_name>/`.

---

## ğŸ“Š Evaluating and Aggregating Results

Once your sweeps finish, use the evaluation toolkit to summarize results:

```powershell
python -m birdnet_custom_classifier_suite.eval_toolkit.aggregate experiments/ --out stage4sweep.csv
python -m birdnet_custom_classifier_suite.eval_toolkit.review stage4sweep.csv
```

This produces a combined CSV of all runs and can rank configs by metrics such as  
`ood.best_f1.f1`, precision, or recall.

---

## ğŸ§¾ Version Control Recommendations

- **Track:**  
  - `config/base.yaml`  
  - all `config/sweep_specs/*.yaml`  
  - `scripts/setup_env.ps1`  
  - everything inside `birdnet_custom_classifier_suite/`

- **Ignore:**  
  - generated `config/sweeps/**`  
  - model and experiment outputs under `experiments/**`  
  - local audio or dataset files (`AudioData/**`)

---

## ğŸ§ª Quick Test Sweep

You can validate your environment with:

```powershell
python -m birdnet_custom_classifier_suite.sweeps.sweep_generator --spec config/sweep_specs/test_sweep.yaml
python -m birdnet_custom_classifier_suite.sweeps.run_sweep config/sweeps/test_sweep --base-config config/base.yaml --verbose
```

This runs a 4-config micro sweep (1 epoch each) to verify your setup end-to-end.

---

## ğŸ§© Future Additions
- Automatic sweep aggregation and leaderboard ranking  
- YAML validation schema  
- CLI presets for common stage types (e.g. â€œStage 4 Robustnessâ€)  
- Cross-platform setup scripts (Linux/macOS)

---

**Maintainer:**  
Tyler Schwenk  
BirdNET-CustomClassifierSuite (2025)
